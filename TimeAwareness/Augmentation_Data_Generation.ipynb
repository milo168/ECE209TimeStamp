{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmenting data for training by shifting IMU\n",
    "\n",
    "# We generate augmented data with\n",
    "- 50ms, 100ms, 150ms, 200ms, 500ms, and 1000ms of shifts.\n",
    "- The shifting is done in the IMU modality.\n",
    "- Training labels are are aligned with the Audio/Video Modality.\n",
    "- The shift are saved in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import *\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, LSTM, Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.core import Permute, Reshape\n",
    "from keras import backend as K\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image as img_PIL\n",
    "\n",
    "\n",
    "# random seed.\n",
    "rand_seed = 2\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(rand_seed)\n",
    "from tqdm.keras import TqdmCallback\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(rand_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='C:/Users/macro/Desktop/UCLA Class/ECE 209AS ML/TimeAwareness/data/'\n",
    "\n",
    "def get_train_and_validate_data(path=path):\n",
    "    Train_data = np.load(path+'Data_train_all.npz')\n",
    "    Labels = Train_data['arr_1']\n",
    "    Features_imu = np.asarray(Train_data['arr_0'],dtype=np.float64)\n",
    "    Features_audio = Train_data['arr_2']\n",
    "    Features_video = Train_data['arr_3']\n",
    "    \n",
    "    randomize = np.arange(len(Labels))\n",
    "    np.random.shuffle(randomize)\n",
    "    \n",
    "    Labels = Labels[randomize]\n",
    "    Features_imu = Features_imu[randomize]\n",
    "    Features_audio = Features_audio[randomize]\n",
    "    Features_video = Features_video[randomize]\n",
    "    \n",
    "    Features_imu = Features_imu.reshape(Features_imu.shape[0],1, Features_imu.shape[1], Features_imu.shape[2]) \n",
    "    \n",
    "    new_Labels = np.split(Labels,[Labels.shape[0] - 2000])\n",
    "    new_Features_imu = np.split(Features_imu,[Features_imu.shape[0] - 2000])\n",
    "    new_Features_audio = np.split(Features_audio,[Features_audio.shape[0] - 2000])\n",
    "    new_Features_video = np.split(Features_video,[Features_video.shape[0] - 2000])\n",
    "    \n",
    "    return new_Labels[0],new_Features_imu[0],new_Features_audio[0],new_Features_video[0],new_Labels[1],new_Features_imu[1],new_Features_audio[1],new_Features_video[1]\n",
    "\n",
    "\n",
    "def get_test_data(path=path):\n",
    "    Train_data = np.load(path+'Data_test_71.pkl',allow_pickle=True)\n",
    "    Labels = np.asarray(Train_data[1])\n",
    "    Features_imu = np.asarray(Train_data[0],dtype=np.float64)\n",
    "    Features_audio = Train_data[2]\n",
    "    Features_video = Train_data[3]\n",
    "    \n",
    "    randomize = np.arange(len(Labels))\n",
    "    np.random.shuffle(randomize)\n",
    "    \n",
    "    Labels = Labels[randomize]\n",
    "    Features_imu = Features_imu[randomize]\n",
    "    Features_audio = Features_audio[randomize]\n",
    "    Features_video = Features_video[randomize]\n",
    "    \n",
    "    Features_imu = Features_imu.reshape(Features_imu.shape[0],1, Features_imu.shape[1], Features_imu.shape[2]) \n",
    "\n",
    "    return Labels,Features_imu,Features_audio,Features_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9976,) (9976, 1, 40, 12) (9976, 193) (9976, 45, 64, 64, 3)\n",
      "(2000,) (2000, 1, 40, 12) (2000, 193) (2000, 45, 64, 64, 3)\n",
      "(1377,) (1377, 1, 40, 12) (1377, 193) (1377, 45, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "Labels,Features_imu,Features_audio,Features_video,Labels2,Features_imu2,Features_audio2,Features_video2 = get_train_and_validate_data()\n",
    "Labels3,Features_imu3,Features_audio3,Features_video3 = get_test_data()\n",
    "\n",
    "print(Labels.shape,Features_imu.shape,Features_audio.shape,Features_video.shape)\n",
    "print(Labels2.shape,Features_imu2.shape,Features_audio2.shape,Features_video2.shape)\n",
    "print(Labels3.shape,Features_imu3.shape,Features_audio3.shape,Features_video3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation: All of these test labels are continuous.\n",
    "### Time window used to create a sample: 2 Second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time shifting, our goal is to use the longest windows and insert time shifting into them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First preprocess the data to get the continuous acc and gyro windows\n",
    "- data ordering: 12 sensors each with 40 samples.\n",
    "- 12 sensors are: acc_right, gyro_right, acc_left, gyro_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['handwashing' 'jump' 'downstair' 'exercise' 'jump' 'handwashing' 'jump'\n",
      " 'walk' 'run' 'upstair' 'exercise' 'upstair' 'upstair' 'upstair' 'upstair'\n",
      " 'walk' 'upstair' 'exercise' 'jump' 'exercise' 'downstair' 'jump'\n",
      " 'exercise' 'downstair' 'upstair' 'run' 'run' 'exercise' 'exercise'\n",
      " 'upstair' 'downstair' 'exercise' 'downstair' 'jump' 'handwashing'\n",
      " 'downstair' 'upstair' 'run' 'handwashing' 'jump' 'run' 'walk' 'run'\n",
      " 'handwashing' 'jump' 'downstair' 'downstair' 'walk' 'downstair' 'upstair'\n",
      " 'upstair' 'handwashing' 'jump' 'downstair' 'downstair' 'upstair' 'jump'\n",
      " 'upstair' 'upstair' 'downstair' 'jump' 'downstair' 'downstair' 'jump'\n",
      " 'upstair' 'downstair' 'downstair' 'jump' 'upstair' 'downstair'\n",
      " 'handwashing' 'handwashing' 'run' 'walk' 'upstair' 'downstair' 'walk'\n",
      " 'downstair' 'upstair' 'downstair' 'handwashing' 'handwashing' 'run'\n",
      " 'upstair' 'handwashing' 'jump' 'handwashing' 'run' 'downstair' 'jump'\n",
      " 'jump' 'run' 'exercise' 'run' 'upstair' 'handwashing' 'downstair' 'run'\n",
      " 'jump' 'exercise']\n",
      "['downstair' 'handwashing' 'run' 'walk' 'walk' 'downstair' 'upstair'\n",
      " 'downstair' 'jump' 'run' 'walk' 'run' 'jump' 'exercise' 'jump' 'run'\n",
      " 'jump' 'walk' 'exercise' 'handwashing' 'handwashing' 'walk' 'upstair'\n",
      " 'upstair' 'run' 'exercise' 'jump' 'downstair' 'exercise' 'upstair' 'walk'\n",
      " 'upstair' 'upstair' 'walk' 'walk' 'downstair' 'downstair' 'upstair'\n",
      " 'handwashing' 'jump' 'jump' 'walk' 'downstair' 'handwashing' 'exercise'\n",
      " 'upstair' 'walk' 'upstair' 'walk' 'handwashing' 'upstair' 'run' 'jump'\n",
      " 'jump' 'jump' 'upstair' 'downstair' 'walk' 'downstair' 'exercise' 'jump'\n",
      " 'jump' 'jump' 'handwashing' 'downstair' 'downstair' 'exercise' 'jump'\n",
      " 'downstair' 'exercise' 'exercise' 'jump' 'downstair' 'walk' 'handwashing'\n",
      " 'run' 'upstair' 'exercise' 'walk' 'run' 'run' 'jump' 'upstair' 'walk'\n",
      " 'walk' 'jump' 'downstair' 'walk' 'run' 'upstair' 'upstair' 'walk' 'jump'\n",
      " 'handwashing' 'handwashing' 'walk' 'walk' 'downstair' 'run' 'upstair']\n",
      "['handwashing' 'jump' 'upstair' 'upstair' 'upstair' 'downstair' 'upstair'\n",
      " 'exercise' 'downstair' 'upstair' 'exercise' 'downstair' 'handwashing'\n",
      " 'walk' 'walk' 'jump' 'upstair' 'upstair' 'run' 'upstair' 'run' 'exercise'\n",
      " 'run' 'run' 'exercise' 'upstair' 'upstair' 'walk' 'run' 'downstair'\n",
      " 'upstair' 'exercise' 'downstair' 'handwashing' 'exercise' 'upstair'\n",
      " 'handwashing' 'handwashing' 'upstair' 'exercise' 'exercise' 'run'\n",
      " 'upstair' 'exercise' 'upstair' 'downstair' 'walk' 'walk' 'upstair'\n",
      " 'downstair' 'handwashing' 'upstair' 'upstair' 'downstair' 'handwashing'\n",
      " 'downstair' 'handwashing' 'exercise' 'exercise' 'run' 'handwashing'\n",
      " 'downstair' 'upstair' 'handwashing' 'upstair' 'handwashing' 'exercise'\n",
      " 'exercise' 'run' 'handwashing' 'handwashing' 'exercise' 'upstair' 'run'\n",
      " 'upstair' 'upstair' 'exercise' 'exercise' 'downstair' 'handwashing'\n",
      " 'jump' 'run' 'exercise' 'downstair' 'handwashing' 'handwashing'\n",
      " 'downstair' 'upstair' 'jump' 'handwashing' 'jump' 'walk' 'jump'\n",
      " 'handwashing' 'run' 'exercise' 'jump' 'exercise' 'walk' 'jump']\n"
     ]
    }
   ],
   "source": [
    "# We see all of the labels are continuous\n",
    "\n",
    "Labels = np.array(Labels)\n",
    "Labels2 = np.array(Labels2)\n",
    "Labels3 = np.array(Labels3)\n",
    "\n",
    "print(Labels[:100])\n",
    "print(Labels2[:100])\n",
    "print(Labels3[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder the labels so they are continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_labels(y_data):\n",
    "    Mapping=dict()\n",
    "    sub_dirs=['downstair','upstair','run','jump','walk','handwashing','exercise']\n",
    "\n",
    "    categories=10\n",
    "\n",
    "    count=0\n",
    "    for i in sub_dirs:\n",
    "        Mapping[i]=count\n",
    "        count=count+1\n",
    "\n",
    "    y_features2=[]\n",
    "    for i in range(len(y_data)):\n",
    "        Type=y_data[i]\n",
    "        lab=Mapping[Type]\n",
    "        y_features2.append(lab)\n",
    "\n",
    "    y_features=np.array(y_features2, dtype=np.int32)\n",
    "    #y_features=y_features.reshape(y_features.shape[0],1)\n",
    "    return y_features.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 3 0 6 3 5 3 4 2 1]\n",
      "[0 5 2 4 4 0 1 0 3 2]\n",
      "[5 3 1 1 1 0 1 6 0 1]\n"
     ]
    }
   ],
   "source": [
    "Labels = number_labels(Labels)\n",
    "Labels2 = number_labels(Labels2)\n",
    "Labels3 = number_labels(Labels3)\n",
    "    \n",
    "print(Labels[:10])\n",
    "print(Labels2[:10])\n",
    "print(Labels3[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 6 6 6]\n",
      "[0 0 0 ... 6 6 6]\n",
      "[0 0 0 ... 6 6 6]\n"
     ]
    }
   ],
   "source": [
    "# Get a sorting order for the labels\n",
    "train_order = np.argsort(Labels)\n",
    "validate_order = np.argsort(Labels2)\n",
    "test_order = np.argsort(Labels3)\n",
    "    \n",
    "print(Labels[train_order])\n",
    "print(Labels2[validate_order])\n",
    "print(Labels3[test_order])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use this sorting order to recreate the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_imu = []\n",
    "Train_labels = []\n",
    "Train_audio = []\n",
    "Train_video = []\n",
    "        \n",
    "for i in range(Labels.shape[0]):\n",
    "    Train_imu.append(Features_imu[train_order[i]])\n",
    "    Train_labels.append(Labels[train_order[i]])\n",
    "    Train_audio.append(Features_audio[train_order[i]])\n",
    "    Train_video.append(Features_video[train_order[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Validate_imu = []\n",
    "Validate_labels = []\n",
    "Validate_audio = []\n",
    "Validate_video = []\n",
    "        \n",
    "for i in range(Labels2.shape[0]):\n",
    "    Validate_imu.append(Features_imu2[validate_order[i]])\n",
    "    Validate_labels.append(Labels2[validate_order[i]])\n",
    "    Validate_audio.append(Features_audio2[validate_order[i]])\n",
    "    Validate_video.append(Features_video2[validate_order[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_imu = []\n",
    "Test_labels = []\n",
    "Test_audio = []\n",
    "Test_video = []\n",
    "        \n",
    "for i in range(Labels3.shape[0]):\n",
    "    Test_imu.append(Features_imu3[test_order[i]])\n",
    "    Test_labels.append(Labels3[test_order[i]])\n",
    "    Test_audio.append(Features_audio3[test_order[i]])\n",
    "    Test_video.append(Features_video3[test_order[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9976, 1, 40, 12) (9976,) (9976, 193) (9976, 45, 64, 64, 3)\n",
      "(2000, 1, 40, 12) (2000,) (2000, 193) (2000, 45, 64, 64, 3)\n",
      "(1377, 1, 40, 12) (1377,) (1377, 193) (1377, 45, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "Train_imu = np.array(Train_imu,dtype=np.float64)\n",
    "Train_labels = np.array(Train_labels)\n",
    "Train_audio = np.array(Train_audio)\n",
    "Train_video = np.array(Train_video)\n",
    "    \n",
    "Validate_imu = np.array(Validate_imu,dtype=np.float64)\n",
    "Validate_labels = np.array(Validate_labels)\n",
    "Validate_audio = np.array(Validate_audio)\n",
    "Validate_video = np.array(Validate_video)\n",
    "    \n",
    "Test_imu = np.array(Test_imu,dtype=np.float64)\n",
    "Test_labels = np.array(Test_labels)\n",
    "Test_audio = np.array(Test_audio)\n",
    "Test_video = np.array(Test_video)   \n",
    "\n",
    "print(Train_imu.shape, Train_labels.shape, Train_audio.shape, Train_video.shape)\n",
    "print(Validate_imu.shape, Validate_labels.shape, Validate_audio.shape, Validate_video.shape)\n",
    "print(Test_imu.shape, Test_labels.shape, Test_audio.shape, Test_video.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 6 6 6]\n",
      "[0 0 0 ... 6 6 6]\n",
      "[0 0 0 ... 6 6 6]\n"
     ]
    }
   ],
   "source": [
    "print(Train_labels)\n",
    "print(Validate_labels)\n",
    "print(Test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "Train_labels = to_categorical(Train_labels)\n",
    "Validate_labels = to_categorical(Validate_labels)\n",
    "Test_labels = to_categorical(Test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/312 [==============================] - 33s 104ms/step - loss: 0.0319 - accuracy: 0.9919\n",
      "63/63 [==============================] - 7s 104ms/step - loss: 0.0254 - accuracy: 0.9945\n",
      "44/44 [==============================] - 4s 101ms/step - loss: 0.2310 - accuracy: 0.9528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.23102378845214844, 0.9527959227561951]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test accuracy of model on the newly concatenated data so we don't mess up the things\n",
    "\n",
    "from keras.models import load_model\n",
    "model_path = 'Baseline Models/imu_video_combined_model_checkpoint'\n",
    "model = load_model(model_path)\n",
    "\n",
    "\n",
    "model.evaluate([Train_imu,Train_video],Train_labels)\n",
    "model.evaluate([Validate_imu,Validate_video],Validate_labels)\n",
    "model.evaluate([Test_imu,Test_video],Test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing accuracy on Reshuffled data.\n",
    "## Note: not time errors are introduced so Accuracy should be same as the fusion model on original continuous test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now aggregating the data and doing the timing errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_acc_right = []\n",
    "Train_gyro_right = []\n",
    "Train_acc_left = []\n",
    "Train_gyro_left = []\n",
    "\n",
    "for i in range(Train_imu.shape[0]):\n",
    "    Train_acc_right.append([Train_imu[i,0,:,0],Train_imu[i,0,:,1],Train_imu[i,0,:,2]])\n",
    "    Train_gyro_right.append([Train_imu[i,0,:,3],Train_imu[i,0,:,4],Train_imu[i,0,:,5]])\n",
    "    Train_acc_left.append([Train_imu[i,0,:,6],Train_imu[i,0,:,7],Train_imu[i,0,:,8]])\n",
    "    Train_gyro_left.append([Train_imu[i,0,:,9],Train_imu[i,0,:,10],Train_imu[i,0,:,11]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Validate_acc_right = []\n",
    "Validate_gyro_right = []\n",
    "Validate_acc_left = []\n",
    "Validate_gyro_left = []\n",
    "\n",
    "for i in range(Validate_imu.shape[0]):\n",
    "    Validate_acc_right.append([Validate_imu[i,0,:,0],Validate_imu[i,0,:,1],Validate_imu[i,0,:,2]])\n",
    "    Validate_gyro_right.append([Validate_imu[i,0,:,3],Validate_imu[i,0,:,4],Validate_imu[i,0,:,5]])\n",
    "    Validate_acc_left.append([Validate_imu[i,0,:,6],Validate_imu[i,0,:,7],Validate_imu[i,0,:,8]])\n",
    "    Validate_gyro_left.append([Validate_imu[i,0,:,9],Validate_imu[i,0,:,10],Validate_imu[i,0,:,11]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_acc_right = []\n",
    "Test_gyro_right = []\n",
    "Test_acc_left = []\n",
    "Test_gyro_left = []\n",
    "\n",
    "for i in range(Test_imu.shape[0]):\n",
    "    Test_acc_right.append([Test_imu[i,0,:,0],Test_imu[i,0,:,1],Test_imu[i,0,:,2]])\n",
    "    Test_gyro_right.append([Test_imu[i,0,:,3],Test_imu[i,0,:,4],Test_imu[i,0,:,5]])\n",
    "    Test_acc_left.append([Test_imu[i,0,:,6],Test_imu[i,0,:,7],Test_imu[i,0,:,8]])\n",
    "    Test_gyro_left.append([Test_imu[i,0,:,9],Test_imu[i,0,:,10],Test_imu[i,0,:,11]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9976, 3, 40)\n",
      "(9976, 3, 40)\n",
      "(9976, 3, 40)\n",
      "(9976, 3, 40)\n",
      "(2000, 3, 40)\n",
      "(2000, 3, 40)\n",
      "(2000, 3, 40)\n",
      "(2000, 3, 40)\n",
      "(1377, 3, 40)\n",
      "(1377, 3, 40)\n",
      "(1377, 3, 40)\n",
      "(1377, 3, 40)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Train_acc_right = np.array(Train_acc_right)\n",
    "Train_gyro_right = np.array(Train_gyro_right)\n",
    "Train_acc_left = np.array(Train_acc_left)\n",
    "Train_gyro_left = np.array(Train_gyro_left)\n",
    "    \n",
    "Validate_acc_right = np.array(Validate_acc_right)\n",
    "Validate_gyro_right = np.array(Validate_gyro_right)\n",
    "Validate_acc_left = np.array(Validate_acc_left)\n",
    "Validate_gyro_left = np.array(Validate_gyro_left)\n",
    "\n",
    "Test_acc_right = np.array(Test_acc_right)\n",
    "Test_gyro_right = np.array(Test_gyro_right)\n",
    "Test_acc_left = np.array(Test_acc_left)\n",
    "Test_gyro_left = np.array(Test_gyro_left)\n",
    "    \n",
    "print(Train_acc_right.shape)\n",
    "print(Train_gyro_right.shape)\n",
    "print(Train_acc_left.shape)\n",
    "print(Train_gyro_left.shape)\n",
    "    \n",
    "print(Validate_acc_right.shape)\n",
    "print(Validate_gyro_right.shape)\n",
    "print(Validate_acc_left.shape)\n",
    "print(Validate_gyro_left.shape)\n",
    "\n",
    "print(Test_acc_right.shape)\n",
    "print(Test_gyro_right.shape)\n",
    "print(Test_acc_left.shape)\n",
    "print(Test_gyro_left.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "Train_acc_right_cont = Train_acc_right[0,]\n",
    "Train_gyro_right_cont = Train_gyro_right[0,]\n",
    "Train_acc_left_cont = Train_acc_left[0,]\n",
    "Train_gyro_left_cont = Train_gyro_left[0,]\n",
    "\n",
    "for i in range(1, Features_imu.shape[0]):\n",
    "    Train_acc_right_cont = np.hstack((Train_acc_right_cont,Train_acc_right[i,]))\n",
    "    Train_gyro_right_cont = np.hstack((Train_gyro_right_cont,Train_gyro_right[i,]))\n",
    "    Train_acc_left_cont = np.hstack((Train_acc_left_cont,Train_acc_left[i,]))\n",
    "    Train_gyro_left_cont = np.hstack((Train_gyro_left_cont,Train_gyro_left[i,]))\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "Validate_acc_right_cont = Validate_acc_right[0,]\n",
    "Validate_gyro_right_cont = Validate_gyro_right[0,]\n",
    "Validate_acc_left_cont = Validate_acc_left[0,]\n",
    "Validate_gyro_left_cont = Validate_gyro_left[0,]\n",
    "\n",
    "\n",
    "for i in range(1, Features_imu2.shape[0]):\n",
    "    Validate_acc_right_cont = np.hstack((Validate_acc_right_cont,Validate_acc_right[i,]))\n",
    "    Validate_gyro_right_cont = np.hstack((Validate_gyro_right_cont,Validate_gyro_right[i,]))\n",
    "    Validate_acc_left_cont = np.hstack((Validate_acc_left_cont,Validate_acc_left[i,]))\n",
    "    Validate_gyro_left_cont = np.hstack((Validate_gyro_left_cont,Validate_gyro_left[i,]))\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "Test_acc_right_cont = Test_acc_right[0,]\n",
    "Test_gyro_right_cont = Test_gyro_right[0,]\n",
    "Test_acc_left_cont = Test_acc_left[0,]\n",
    "Test_gyro_left_cont = Test_gyro_left[0,]\n",
    "\n",
    "for i in range(1, Features_imu3.shape[0]):\n",
    "    Test_acc_right_cont = np.hstack((Test_acc_right_cont,Test_acc_right[i,]))\n",
    "    Test_gyro_right_cont = np.hstack((Test_gyro_right_cont,Test_gyro_right[i,]))\n",
    "    Test_acc_left_cont = np.hstack((Test_acc_left_cont,Test_acc_left[i,]))\n",
    "    Test_gyro_left_cont = np.hstack((Test_gyro_left_cont,Test_gyro_left[i,]))\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 399040)\n",
      "(3, 399040)\n",
      "(3, 399040)\n",
      "(3, 399040)\n",
      "(3, 80000)\n",
      "(3, 80000)\n",
      "(3, 80000)\n",
      "(3, 80000)\n",
      "(3, 55080)\n",
      "(3, 55080)\n",
      "(3, 55080)\n",
      "(3, 55080)\n"
     ]
    }
   ],
   "source": [
    "print(Train_acc_right_cont.shape)\n",
    "print(Train_gyro_right_cont.shape)\n",
    "print(Train_acc_left_cont.shape)\n",
    "print(Train_gyro_left_cont.shape)\n",
    "    \n",
    "\n",
    "print(Validate_acc_right_cont.shape)\n",
    "print(Validate_gyro_right_cont.shape)\n",
    "print(Validate_acc_left_cont.shape)\n",
    "print(Validate_gyro_left_cont.shape)\n",
    "    \n",
    "\n",
    "print(Test_acc_right_cont.shape)\n",
    "print(Test_gyro_right_cont.shape)\n",
    "print(Test_acc_left_cont.shape)\n",
    "print(Test_gyro_left_cont.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now shifting the samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining function to do the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_sample_shift(acc_right_cont, gyro_right_cont, acc_left_cont, gyro_left_cont, audio, labels, video, shift_samples = 1):\n",
    "    sample_size = 40 #need to be 40, as decided during training\n",
    "    total_samples = acc_right_cont.shape[1]\n",
    "    \n",
    "    #print(total_samples)\n",
    "    \n",
    "    current_cursor = shift_samples\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    acc_right_pro= np.array(acc_right_cont[np.newaxis,:,current_cursor:current_cursor+sample_size])\n",
    "\n",
    "    #print(acc_right_pro.shape)\n",
    "\n",
    "    gyro_right_pro= np.array(gyro_right_cont[np.newaxis,:,current_cursor:current_cursor+sample_size])\n",
    "\n",
    "    acc_left_pro= np.array(acc_left_cont[np.newaxis,:,current_cursor:current_cursor+sample_size])\n",
    "\n",
    "    gyro_left_pro= np.array(gyro_left_cont[np.newaxis,:,current_cursor:current_cursor+sample_size])\n",
    "\n",
    "    #print(np.array(acc_right_cont[np.newaxis,:,current_cursor:current_cursor+sample_size]).shape)\n",
    "\n",
    "    while current_cursor<=(total_samples-2*sample_size):\n",
    "        current_cursor = current_cursor + sample_size\n",
    "        #print(current_cursor,\" : \", i)\n",
    "        a=acc_right_pro\n",
    "        b=np.array(acc_right_cont[np.newaxis,:,current_cursor:current_cursor+sample_size])\n",
    "        acc_right_pro = np.concatenate((a,b),axis=0)\n",
    "\n",
    "\n",
    "        a=gyro_right_pro\n",
    "        b=np.array(gyro_right_cont[np.newaxis,:,current_cursor:current_cursor+sample_size])\n",
    "        gyro_right_pro = np.concatenate((a,b),axis=0)\n",
    "\n",
    "\n",
    "        a=acc_left_pro\n",
    "        b=np.array(acc_left_cont[np.newaxis,:,current_cursor:current_cursor+sample_size])\n",
    "        #print(a.shape,b.shape)\n",
    "        acc_left_pro = np.concatenate((a,b),axis=0)\n",
    "\n",
    "        a=gyro_left_pro\n",
    "        b=np.array(gyro_left_cont[np.newaxis,:,current_cursor:current_cursor+sample_size])\n",
    "        gyro_left_pro = np.concatenate((a,b),axis=0)\n",
    "        i = i+1\n",
    "        \n",
    "    IMU_processed = np.concatenate((acc_right_pro,gyro_right_pro,acc_left_pro,gyro_left_pro),axis=1)\n",
    "    IMU_processed = IMU_processed[:,np.newaxis,:,:]\n",
    "    IMU_processed = np.swapaxes(IMU_processed,2,3)\n",
    "        \n",
    "    size = IMU_processed.shape[0]    \n",
    "        \n",
    "    return audio[:size],IMU_processed, labels[:size], video[:size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating more training datasets by doing shifting\n",
    "- 1 sample shift = 50 ms timing error.\n",
    "- 20 sample shift = 1000ms timing error.\n",
    "- 1000ms augmentation uses shifts (1,2,10,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9975, 193) (9975, 1, 40, 12) (9975, 7) (9975, 45, 64, 64, 3)\n",
      "(1999, 193) (1999, 1, 40, 12) (1999, 7) (1999, 45, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "#shift by 1 sample\n",
    "Train_Features_audio_1, Train_IMU_1 , Train_Labels_1, Train_Features_video_1 = get_data_sample_shift(Train_acc_right_cont, Train_gyro_right_cont, Train_acc_left_cont, Train_gyro_left_cont, Features_audio, Train_labels, Features_video, shift_samples = 1)\n",
    "Validate_Features_audio_1, Validate_IMU_1 , Validate_Labels_1, Validate_Features_video_1 = get_data_sample_shift(Validate_acc_right_cont, Validate_gyro_right_cont, Validate_acc_left_cont, Validate_gyro_left_cont, Features_audio2, Validate_labels, Features_video2, shift_samples = 1)\n",
    "\n",
    "print(Train_Features_audio_1.shape, Train_IMU_1.shape, Train_Labels_1.shape, Train_Features_video_1.shape)\n",
    "print(Validate_Features_audio_1.shape, Validate_IMU_1.shape, Validate_Labels_1.shape, Validate_Features_video_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "path_1_Train = 'E:/augmented_data/train_data_1_shift'\n",
    "path_1_Validate = 'E:/augmented_data/validate_data_1_shift'\n",
    "\n",
    "np.savez(path_1_Train, Train_IMU_1, Train_Labels_1, Train_Features_audio_1, Train_Features_video_1)\n",
    "np.savez(path_1_Validate, Validate_IMU_1, Validate_Labels_1, Validate_Features_audio_1, Validate_Features_video_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9975, 193) (9975, 1, 40, 12) (9975, 7) (9975, 45, 64, 64, 3)\n",
      "(1999, 193) (1999, 1, 40, 12) (1999, 7) (1999, 45, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "#shift by 2 sample\n",
    "Train_Features_audio_2, Train_IMU_2 , Train_Labels_2, Train_Features_video_2 = get_data_sample_shift(Train_acc_right_cont, Train_gyro_right_cont, Train_acc_left_cont, Train_gyro_left_cont, Features_audio, Train_labels, Features_video, shift_samples = 2)\n",
    "Validate_Features_audio_2, Validate_IMU_2 , Validate_Labels_2, Validate_Features_video_2 = get_data_sample_shift(Validate_acc_right_cont, Validate_gyro_right_cont, Validate_acc_left_cont, Validate_gyro_left_cont, Features_audio2, Validate_labels, Features_video2, shift_samples = 2)\n",
    "\n",
    "print(Train_Features_audio_2.shape, Train_IMU_2.shape, Train_Labels_2.shape, Train_Features_video_2.shape)\n",
    "print(Validate_Features_audio_2.shape, Validate_IMU_2.shape, Validate_Labels_2.shape, Validate_Features_video_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "path_2_Train = 'E:/augmented_data/train_data_2_shift'\n",
    "path_2_Validate = 'E:/augmented_data/validate_data_2_shift'\n",
    "\n",
    "np.savez(path_2_Train, Train_IMU_2, Train_Labels_2, Train_Features_audio_2, Train_Features_video_2)\n",
    "np.savez(path_2_Validate, Validate_IMU_2, Validate_Labels_2, Validate_Features_audio_2, Validate_Features_video_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9975, 193) (9975, 1, 40, 12) (9975, 7) (9975, 45, 64, 64, 3)\n",
      "(1999, 193) (1999, 1, 40, 12) (1999, 7) (1999, 45, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "#shift by 10 sample\n",
    "Train_Features_audio_10, Train_IMU_10 , Train_Labels_10, Train_Features_video_10 = get_data_sample_shift(Train_acc_right_cont, Train_gyro_right_cont, Train_acc_left_cont, Train_gyro_left_cont, Features_audio, Train_labels, Features_video, shift_samples = 10)\n",
    "Validate_Features_audio_10, Validate_IMU_10 , Validate_Labels_10, Validate_Features_video_10 = get_data_sample_shift(Validate_acc_right_cont, Validate_gyro_right_cont, Validate_acc_left_cont, Validate_gyro_left_cont, Features_audio2, Validate_labels, Features_video2, shift_samples = 10)\n",
    "\n",
    "print(Train_Features_audio_10.shape, Train_IMU_10.shape, Train_Labels_10.shape, Train_Features_video_10.shape)\n",
    "print(Validate_Features_audio_10.shape, Validate_IMU_10.shape, Validate_Labels_10.shape, Validate_Features_video_10.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "path_10_Train = 'E:/augmented_data/train_data_10_shift'\n",
    "path_10_Validate = 'E:/augmented_data/validate_data_10_shift'\n",
    "\n",
    "np.savez(path_10_Train, Train_IMU_10, Train_Labels_10, Train_Features_audio_10, Train_Features_video_10)\n",
    "np.savez(path_10_Validate, Validate_IMU_10, Validate_Labels_10, Validate_Features_audio_10, Validate_Features_video_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9975, 193) (9975, 1, 40, 12) (9975, 7) (9975, 45, 64, 64, 3)\n",
      "(1999, 193) (1999, 1, 40, 12) (1999, 7) (1999, 45, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "#shift by 20 sample\n",
    "Train_Features_audio_20, Train_IMU_20 , Train_Labels_20, Train_Features_video_20 = get_data_sample_shift(Train_acc_right_cont, Train_gyro_right_cont, Train_acc_left_cont, Train_gyro_left_cont, Features_audio, Train_labels, Features_video, shift_samples = 20)\n",
    "Validate_Features_audio_20, Validate_IMU_20 , Validate_Labels_20, Validate_Features_video_20 = get_data_sample_shift(Validate_acc_right_cont, Validate_gyro_right_cont, Validate_acc_left_cont, Validate_gyro_left_cont, Features_audio2, Validate_labels, Features_video2, shift_samples = 20)\n",
    "\n",
    "print(Train_Features_audio_20.shape, Train_IMU_20.shape, Train_Labels_20.shape, Train_Features_video_20.shape)\n",
    "print(Validate_Features_audio_20.shape, Validate_IMU_20.shape, Validate_Labels_20.shape, Validate_Features_video_20.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "path_20_Train = 'E:/augmented_data/train_data_20_shift'\n",
    "path_20_Validate = 'E:/augmented_data/validate_data_20_shift'\n",
    "\n",
    "np.savez(path_20_Train, Train_IMU_20, Train_Labels_20, Train_Features_audio_20, Train_Features_video_20)\n",
    "np.savez(path_20_Validate, Validate_IMU_20, Validate_Labels_20, Validate_Features_audio_20, Validate_Features_video_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,41):\n",
    "    Test_Features_audio_shiftAmount,Test_IMU_shiftAmount, Test_Labels_shiftAmount,Test_Features_video_shiftAmount = get_data_sample_shift(Test_acc_right_cont, Test_gyro_right_cont, Test_acc_left_cont, Test_gyro_left_cont, Features_audio3, Test_labels, Features_video3, shift_samples = i)\n",
    "    path_shiftAmount_Test = 'E:/augmented_data/test_data_'+str(i)+'_shift'\n",
    "    np.savez(path_shiftAmount_Test, Test_IMU_shiftAmount, Test_Labels_shiftAmount, Test_Features_audio_shiftAmount, Test_Features_video_shiftAmount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
